{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        self.feature_dim = 256\n",
    "        self.pred_dim = 256\n",
    "        self.learning_rate = 0.05\n",
    "        self.momentum = 0.99\n",
    "        self.weight_decay = 1e-4\n",
    "        self.epochs = 20\n",
    "        self.device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_crop(image, size=[28, 28, 1])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image, label\n",
    "\n",
    "class Encoder(keras.Model):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = keras.Sequential([\n",
    "            layers.Conv2D(64, 3, 1, 'same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(128, 3, 1, 'same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(2),\n",
    "            layers.Conv2D(256, 3, 1, 'same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(2),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1024, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(feature_dim),\n",
    "            layers.BatchNormalization()\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Predictor(keras.Model):\n",
    "    def __init__(self, feature_dim, pred_dim):\n",
    "        super().__init__()\n",
    "        self.predictor = keras.Sequential([\n",
    "            layers.Dense(pred_dim, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(feature_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.predictor(x)\n",
    "\n",
    "class DirectPred(keras.Model):\n",
    "    def __init__(self, feature_dim, pred_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(feature_dim)\n",
    "        self.predictor = Predictor(feature_dim, pred_dim)\n",
    "        self.target_encoder = Encoder(feature_dim)\n",
    "\n",
    "        # Build the models\n",
    "        dummy_input = tf.keras.Input(shape=(28, 28, 1))\n",
    "        self.encoder(dummy_input)\n",
    "        self.predictor(self.encoder(dummy_input))\n",
    "        self.target_encoder(dummy_input)\n",
    "\n",
    "        # Initialize target_encoder with encoder's parameters\n",
    "        self.target_encoder.set_weights(self.encoder.get_weights())\n",
    "\n",
    "    def call(self, x1, x2):\n",
    "        z1 = self.encoder(x1)\n",
    "        z2 = self.encoder(x2)\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        with tf.stop_gradient(self.target_encoder):\n",
    "            t1 = self.target_encoder(x1)\n",
    "            t2 = self.target_encoder(x2)\n",
    "        return p1, p2, t1, t2\n",
    "\n",
    "@tf.function\n",
    "def directpred_loss(p1, p2, t1, t2):\n",
    "    loss = tf.reduce_mean(tf.square(p1 - t2)) + tf.reduce_mean(tf.square(p2 - t1))\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, x1, x2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        p1, p2, t1, t2 = model(x1, x2)\n",
    "        loss = directpred_loss(p1, p2, t1, t2)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def update_target_encoder(model, momentum):\n",
    "    for encoder_weight, target_weight in zip(model.encoder.weights, model.target_encoder.weights):\n",
    "        target_weight.assign(momentum * target_weight + (1 - momentum) * encoder_weight)\n",
    "\n",
    "def train(model, train_dataset, optimizer, config):\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for x, _ in tqdm(train_dataset, desc=f\"Epoch {epoch + 1}/{config.epochs}\"):\n",
    "            x1 = augment(x, None)[0]\n",
    "            x2 = augment(x, None)[0]\n",
    "            loss = train_step(model, optimizer, x1, x2)\n",
    "            update_target_encoder(model, config.momentum)\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            linear_acc, knn_acc = evaluate(model, train_dataset, test_dataset)\n",
    "            print(f\"Linear Evaluation Accuracy: {linear_acc:.4f}\")\n",
    "            print(f\"KNN Evaluation Accuracy: {knn_acc:.4f}\")\n",
    "\n",
    "def extract_features(model, dataset):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for x, y in dataset:\n",
    "        feature = model.encoder(x)\n",
    "        features.append(feature.numpy())\n",
    "        labels.append(y.numpy())\n",
    "    return np.concatenate(features), np.concatenate(labels)\n",
    "\n",
    "def linear_evaluation(train_features, train_labels, test_features, test_labels):\n",
    "    classifier = keras.Sequential([\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    classifier.fit(train_features, train_labels, epochs=100, verbose=0)\n",
    "    _, accuracy = classifier.evaluate(test_features, test_labels, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def knn_evaluation(train_features, train_labels, test_features, test_labels, k=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(train_features, train_labels)\n",
    "    predictions = knn.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(model, train_dataset, test_dataset):\n",
    "    train_features, train_labels = extract_features(model, train_dataset)\n",
    "    test_features, test_labels = extract_features(model, test_dataset)\n",
    "    \n",
    "    linear_acc = linear_evaluation(train_features, train_labels, test_features, test_labels)\n",
    "    knn_acc = knn_evaluation(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    return linear_acc, knn_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(10000).batch(config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    test_dataset = test_dataset.batch(config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    with tf.device(config.device):\n",
    "        model = DirectPred(config.feature_dim, config.pred_dim)\n",
    "        \n",
    "        # Ensure the model is built\n",
    "        dummy_input = tf.keras.Input(shape=(28, 28, 1))\n",
    "        model(dummy_input, dummy_input)\n",
    "\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate, momentum=0.9)\n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate=config.learning_rate,\n",
    "            decay_steps=config.epochs * len(list(train_dataset))\n",
    "        )\n",
    "        optimizer.learning_rate = lr_schedule\n",
    "\n",
    "        print(\"Starting DirectPred training...\")\n",
    "        train(model, train_dataset, optimizer, config)\n",
    "\n",
    "        print(\"\\nFinal Evaluation:\")\n",
    "        linear_acc, knn_acc = evaluate(model, train_dataset, test_dataset)\n",
    "        print(f\"Linear Evaluation Accuracy: {linear_acc:.4f}\")\n",
    "        print(f\"KNN Evaluation Accuracy: {knn_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
