{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "\n",
    "# 1. Data loading and augmentation\n",
    "def create_augmented_dataset(x, y, batch_size):\n",
    "    def augment(image):\n",
    "        image = tf.image.random_crop(image, size=[28, 28, 1])\n",
    "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "        image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "        return image\n",
    "\n",
    "    def create_pair(image, label):\n",
    "        return augment(image), augment(image), label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(create_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# 2. Barlow Twins model architecture\n",
    "class BarlowTwinsMNIST(keras.Model):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(BarlowTwinsMNIST, self).__init__()\n",
    "        self.encoder = keras.Sequential([\n",
    "            layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "            layers.Conv2D(64, 3, activation='relu'),\n",
    "            layers.MaxPooling2D(2),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dense(feature_dim)\n",
    "        ])\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = self.encoder(inputs)\n",
    "        z = self.bn(z)\n",
    "        return z\n",
    "\n",
    "# 3. Barlow Twins loss function\n",
    "@tf.function\n",
    "def barlow_twins_loss(z1, z2, lambda_param=5e-3):\n",
    "    batch_size = tf.shape(z1)[0]\n",
    "    feature_dim = tf.shape(z1)[1]\n",
    "    \n",
    "    c = tf.matmul(z1, z2, transpose_a=True) / tf.cast(batch_size, tf.float32)\n",
    "    c_diff = (c - tf.eye(feature_dim)) ** 2\n",
    "    \n",
    "    off_diagonal = tf.linalg.set_diag(c_diff, tf.zeros(feature_dim))\n",
    "    loss = tf.reduce_sum(tf.linalg.diag_part(c_diff)) + lambda_param * tf.reduce_sum(off_diagonal)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 4. Pre-training function\n",
    "@tf.function\n",
    "def train_step(model, optimizer, x1, x2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z1 = model(x1)\n",
    "        z2 = model(x2)\n",
    "        loss = barlow_twins_loss(z1, z2)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def pretrain_barlow_twins(model, train_dataset, optimizer, epochs=10):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x1, x2, _ in train_dataset:\n",
    "            loss = train_step(model, optimizer, x1, x2)\n",
    "            epoch_loss += loss\n",
    "        avg_loss = epoch_loss / len(train_dataset)\n",
    "        losses.append(avg_loss.numpy())\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n",
    "    return losses\n",
    "\n",
    "# 5. Classifier for fine-tuning\n",
    "class MNISTClassifier(keras.Model):\n",
    "    def __init__(self, encoder):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features = self.encoder(inputs)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# 6. Fine-tuning function\n",
    "@tf.function\n",
    "def train_classifier_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def finetune(model, train_dataset, test_dataset, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_dataset:\n",
    "            loss = train_classifier_step(model, optimizer, x, y)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        for x, y in test_dataset:\n",
    "            y_pred = model(x)\n",
    "            test_accuracy.update_state(y, y_pred)\n",
    "        print(f'Epoch {epoch+1}, Test Accuracy: {test_accuracy.result().numpy():.4f}')\n",
    "\n",
    "# 7. Feature extraction function\n",
    "def extract_features(model, dataset):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for x, y in dataset:\n",
    "        feature = model.encoder(x)\n",
    "        features.append(feature.numpy())\n",
    "        labels.append(y.numpy())\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "# 8. Visualization function\n",
    "def plot_embeddings(features, labels, method='UMAP'):\n",
    "    if method == 'UMAP':\n",
    "        reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    elif method == 'PCA':\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'UMAP' or 'PCA'\")\n",
    "    \n",
    "    embeddings = reducer.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='tab10', s=5, alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'{method} visualization of Barlow Twins learned representations')\n",
    "    plt.xlabel(f'{method}_1')\n",
    "    plt.ylabel(f'{method}_2')\n",
    "    plt.savefig(f'barlow_twins_mnist_{method.lower()}_tf.png')\n",
    "    plt.close()\n",
    "\n",
    "# 9. Main execution\n",
    "def main():\n",
    "    # Load and preprocess MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "\n",
    "    # Create datasets\n",
    "    batch_size = 256\n",
    "    train_dataset_pretraining = create_augmented_dataset(x_train, y_train, batch_size)\n",
    "    \n",
    "    # Create separate datasets for fine-tuning\n",
    "    train_dataset_finetuning = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "    # Initialize model and optimizer for pre-training\n",
    "    model = BarlowTwinsMNIST()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # Pre-train the model\n",
    "    print(\"Starting pre-training...\")\n",
    "    pretrain_losses = pretrain_barlow_twins(model, train_dataset_pretraining, optimizer, epochs=50)\n",
    "\n",
    "    # Plot pre-training loss\n",
    "    plt.plot(pretrain_losses)\n",
    "    plt.title('Barlow Twins Pre-training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('barlow_twins_pretrain_loss_tf.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save the pre-trained model\n",
    "    model.save_weights('barlow_twins_pretrained_mnist_tf.h5')\n",
    "    print(\"Pre-training complete!\")\n",
    "\n",
    "    # Initialize classifier for fine-tuning\n",
    "    classifier = MNISTClassifier(model.encoder)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    finetune(classifier, train_dataset_finetuning, test_dataset, optimizer, epochs=10)\n",
    "\n",
    "    print(\"Fine-tuning complete!\")\n",
    "\n",
    "    # Extract features for visualization\n",
    "    features, labels = extract_features(classifier, test_dataset)\n",
    "\n",
    "    # Plot UMAP\n",
    "    print(\"Generating UMAP visualization...\")\n",
    "    plot_embeddings(features, labels, method='UMAP')\n",
    "\n",
    "    # Plot PCA\n",
    "    print(\"Generating PCA visualization...\")\n",
    "    plot_embeddings(features, labels, method='PCA')\n",
    "\n",
    "    print(\"Visualizations complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
