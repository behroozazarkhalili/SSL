{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.down1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.down2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 3, padding=1)\n",
    "        self.up2 = nn.ConvTranspose2d(64, out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x1 = F.relu(self.down1(x))\n",
    "        x2 = F.relu(self.down2(x1))\n",
    "        x = F.relu(self.up1(x2))\n",
    "        x = self.up2(x + x1)\n",
    "        return x\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "        self.model = SimpleUNet(1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.model(x, t)\n",
    "\n",
    "    def get_loss(self, x_0):\n",
    "        t = torch.randint(0, self.num_timesteps, (x_0.shape[0],))\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t = self.q_sample(x_0, t, noise)\n",
    "        predicted_noise = self(x_t, t)\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "        return loss\n",
    "\n",
    "    def q_sample(self, x_0, t, noise):\n",
    "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "        return torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t):\n",
    "        beta_t = self.beta[t]\n",
    "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - self.alpha_bar[t])\n",
    "        sqrt_alpha_bar_t = torch.sqrt(self.alpha_bar[t])\n",
    "        \n",
    "        model_mean = (1 / torch.sqrt(self.alpha[t])) * (x - (beta_t / sqrt_one_minus_alpha_bar_t) * self(x, t))\n",
    "        \n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            return model_mean + torch.sqrt(beta_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, num_samples, size, device):\n",
    "        x = torch.randn(num_samples, 1, *size).to(device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            x = self.p_sample(x, torch.full((num_samples,), t, device=device, dtype=torch.long))\n",
    "        return x\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, num_epochs, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x = batch[0].to(device)\n",
    "            loss = model.get_loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualization\n",
    "def visualize_samples(model, num_samples, size, device):\n",
    "    samples = model.sample(num_samples, size, device)\n",
    "    samples = samples.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*3, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(samples[i, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DDPM().to(device)\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 10\n",
    "    train(model, dataloader, num_epochs, device)\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_samples(model, num_samples=5, size=(28, 28), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "class ImprovedUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([\n",
    "            Block(64, 128, time_emb_dim),\n",
    "            Block(128, 256, time_emb_dim),\n",
    "            Block(256, 256, time_emb_dim),\n",
    "        ])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([\n",
    "            Block(256, 256, time_emb_dim, up=True),\n",
    "            Block(384, 128, time_emb_dim, up=True),\n",
    "            Block(192, 64, time_emb_dim, up=True),\n",
    "        ])\n",
    "\n",
    "        self.output = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "class ImprovedDDPM(nn.Module):\n",
    "    def __init__(self, num_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.model = ImprovedUNet()\n",
    "\n",
    "        # Cosine noise schedule\n",
    "        betas = cosine_beta_schedule(num_timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        \n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        predicted_noise = self.model(x_noisy, t)\n",
    "\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_index):\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "        )\n",
    "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas_cumprod, t, x.shape)\n",
    "        \n",
    "        # Equation 11 in the paper\n",
    "        # Use our model (noise predictor) to predict the mean\n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * self.model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            # Algorithm 2 line 4:\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape):\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        b = shape[0]\n",
    "        # start from pure noise (for each example in the batch)\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='sampling loop time step', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, image_size, batch_size=16, channels=3):\n",
    "        return self.p_sample_loop(shape=(batch_size, channels, image_size, image_size))\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, num_epochs, device, lr=2e-4):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            x = batch[0].to(device)\n",
    "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            loss = model.p_losses(x, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "# Visualization\n",
    "def visualize_samples(model, num_samples, image_size, channels, device):\n",
    "    model.eval()\n",
    "    samples = model.sample(image_size=image_size, batch_size=num_samples, channels=channels)\n",
    "    samples = torch.from_numpy(samples[-1])  # Get the last step of the sampling process\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*3, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImprovedDDPM().to(device)\n",
    "\n",
    "    # Load CIFAR-10 dataset (or any other dataset you prefer)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 100  # You might want to increase this for better results\n",
    "    train(model, dataloader, num_epochs, device)\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_samples(model, num_samples=5, image_size=32, channels=3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        h = h + time_emb\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.downs = nn.ModuleList([\n",
    "            Block(64, 128, time_emb_dim),\n",
    "            Block(128, 256, time_emb_dim),\n",
    "            Block(256, 256, time_emb_dim),\n",
    "        ])\n",
    "        self.ups = nn.ModuleList([\n",
    "            Block(256, 256, time_emb_dim, up=True),\n",
    "            Block(384, 128, time_emb_dim, up=True),\n",
    "            Block(192, 64, time_emb_dim, up=True),\n",
    "        ])\n",
    "        self.output = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        t = self.time_mlp(timestep)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "class LatentDiffusion(nn.Module):\n",
    "    def __init__(self, autoencoder, unet, num_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.unet = unet\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = cosine_beta_schedule(num_timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        \n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        predicted_noise = self.unet(x_noisy, t)\n",
    "\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_index):\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "        )\n",
    "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas_cumprod, t, x.shape)\n",
    "        \n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * self.unet(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(self.betas, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape):\n",
    "        device = next(self.unet.parameters()).device\n",
    "\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='sampling loop time step', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, image_size, batch_size=16, channels=3):\n",
    "        return self.p_sample_loop(shape=(batch_size, channels, image_size // 8, image_size // 8))\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# Training loop\n",
    "def train(ldm, autoencoder, dataloader, num_epochs, device, lr=1e-4):\n",
    "    optimizer = torch.optim.AdamW(ldm.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        ldm.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            # Encode the input to latent space\n",
    "            with torch.no_grad():\n",
    "                x_latent = autoencoder.encode(x)\n",
    "            \n",
    "            t = torch.randint(0, ldm.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            loss = ldm.p_losses(x_latent, t)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "def visualize_samples(ldm, autoencoder, num_samples, image_size, channels, device):\n",
    "    ldm.eval()\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    latent_samples = ldm.sample(image_size=image_size, batch_size=num_samples, channels=channels)\n",
    "    latent_samples = torch.from_numpy(latent_samples[-1]).to(device)  # Get the last step of the sampling process\n",
    "    \n",
    "    # Decode the latent samples\n",
    "    with torch.no_grad():\n",
    "        samples = autoencoder.decode(latent_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*3, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize models\n",
    "    autoencoder = Autoencoder().to(device)\n",
    "    unet = UNet(in_channels=256, out_channels=256, time_emb_dim=32).to(device)\n",
    "    ldm = LatentDiffusion(autoencoder, unet).to(device)\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder_optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "    num_epochs_ae = 10\n",
    "    for epoch in range(num_epochs_ae):\n",
    "        autoencoder.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Autoencoder Epoch {epoch+1}/{num_epochs_ae}\"):\n",
    "            x = batch[0].to(device)\n",
    "            autoencoder_optimizer.zero_grad()\n",
    "            x_recon = autoencoder(x)\n",
    "            loss = F.mse_loss(x_recon, x)\n",
    "            loss.backward()\n",
    "            autoencoder_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Autoencoder Epoch {epoch+1}/{num_epochs_ae}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Train the latent diffusion model\n",
    "    num_epochs_ldm = 50\n",
    "    train(ldm, autoencoder, dataloader, num_epochs_ldm, device)\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_samples(ldm, autoencoder, num_samples=5, image_size=32, channels=256, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, class_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.class_mlp = nn.Linear(class_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, c):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        class_emb = self.relu(self.class_mlp(c))\n",
    "        h = h + time_emb[(..., ) + (None, ) * 2] + class_emb[(..., ) + (None, ) * 2]\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=32, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.class_emb = nn.Embedding(num_classes, time_emb_dim)\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.downs = nn.ModuleList([\n",
    "            Block(64, 128, time_emb_dim, time_emb_dim),\n",
    "            Block(128, 256, time_emb_dim, time_emb_dim),\n",
    "            Block(256, 256, time_emb_dim, time_emb_dim),\n",
    "        ])\n",
    "        self.ups = nn.ModuleList([\n",
    "            Block(256, 256, time_emb_dim, time_emb_dim, up=True),\n",
    "            Block(384, 128, time_emb_dim, time_emb_dim, up=True),\n",
    "            Block(192, 64, time_emb_dim, time_emb_dim, up=True),\n",
    "        ])\n",
    "        self.output = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, timestep, class_label):\n",
    "        t = self.time_mlp(timestep)\n",
    "        c = self.class_emb(class_label)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t, c)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t, c)\n",
    "        return self.output(x)\n",
    "\n",
    "class ConditionalDDPM(nn.Module):\n",
    "    def __init__(self, model, num_classes, num_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = cosine_beta_schedule(num_timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        \n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, x_start, t, class_labels, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        predicted_noise = self.model(x_noisy, t, class_labels)\n",
    "\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_index, class_labels):\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "        )\n",
    "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas_cumprod, t, x.shape)\n",
    "        \n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * self.model(x, t, class_labels) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(self.betas, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, class_labels):\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='sampling loop time step', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long), i, class_labels)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, image_size, batch_size=16, channels=3, class_labels=None):\n",
    "        if class_labels is None:\n",
    "            class_labels = torch.randint(0, self.num_classes, (batch_size,), device=next(self.model.parameters()).device)\n",
    "        return self.p_sample_loop(shape=(batch_size, channels, image_size, image_size), class_labels=class_labels)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# Training loop\n",
    "def train(cddpm, dataloader, num_epochs, device, lr=2e-4):\n",
    "    optimizer = torch.optim.AdamW(cddpm.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        cddpm.train()\n",
    "        total_loss = 0\n",
    "        for batch, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            x = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            t = torch.randint(0, cddpm.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            loss = cddpm.p_losses(x, t, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "def visualize_samples(cddpm, num_samples, image_size, channels, device, class_labels=None):\n",
    "    cddpm.eval()\n",
    "    if class_labels is None:\n",
    "        class_labels = torch.randint(0, cddpm.num_classes, (num_samples,), device=device)\n",
    "    samples = cddpm.sample(image_size=image_size, batch_size=num_samples, channels=channels, class_labels=class_labels)\n",
    "    samples = torch.from_numpy(samples[-1])  # Get the last step of the sampling process\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*3, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Class: {class_labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = 10  # CIFAR-10 has 10 classes\n",
    "    model = ConditionalUNet(num_classes=num_classes).to(device)\n",
    "    cddpm = ConditionalDDPM(model, num_classes=num_classes).to(device)\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 100  # You might want to increase this for better results\n",
    "    train(cddpm, dataloader, num_epochs, device)\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_samples(cddpm, num_samples=5, image_size=32, channels=3, device=device)\n",
    "\n",
    "    # Generate samples for specific classes\n",
    "    class_labels = torch.tensor([0, 1, 2, 3, 4], device=device)  # Generate one sample for each of the first 5 classes\n",
    "    visualize_samples(cddpm, num_samples=5, image_size=32, channels=3, device=device, class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, class_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.class_mlp = nn.Linear(class_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, c):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        class_emb = self.relu(self.class_mlp(c))\n",
    "        h = h + time_emb[(..., ) + (None, ) * 2] + class_emb[(..., ) + (None, ) * 2]\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "class ConditionalImprovedUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=32, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.class_emb = nn.Embedding(num_classes, time_emb_dim)\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.downs = nn.ModuleList([\n",
    "            Block(64, 128, time_emb_dim, time_emb_dim),\n",
    "            Block(128, 256, time_emb_dim, time_emb_dim),\n",
    "            Block(256, 256, time_emb_dim, time_emb_dim),\n",
    "        ])\n",
    "        self.ups = nn.ModuleList([\n",
    "            Block(256, 256, time_emb_dim, time_emb_dim, up=True),\n",
    "            Block(384, 128, time_emb_dim, time_emb_dim, up=True),\n",
    "            Block(192, 64, time_emb_dim, time_emb_dim, up=True),\n",
    "        ])\n",
    "        self.output = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, timestep, class_label):\n",
    "        t = self.time_mlp(timestep)\n",
    "        c = self.class_emb(class_label)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t, c)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t, c)\n",
    "        return self.output(x)\n",
    "\n",
    "class ConditionalImprovedDDPM(nn.Module):\n",
    "    def __init__(self, model, num_classes, num_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = cosine_beta_schedule(num_timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        \n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "        \n",
    "        # Log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)))\n",
    "        self.register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        self.register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, x_start, t, class_labels, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        predicted_noise = self.model(x_noisy, t, class_labels)\n",
    "\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_index, class_labels):\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "        )\n",
    "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas_cumprod, t, x.shape)\n",
    "        \n",
    "        # Equation 11 in the paper\n",
    "        # Use our model (noise predictor) to predict the mean\n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * self.model(x, t, class_labels) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            # Algorithm 2 line 4:\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, class_labels):\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='sampling loop time step', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long), i, class_labels)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, image_size, batch_size=16, channels=3, class_labels=None):\n",
    "        if class_labels is None:\n",
    "            class_labels = torch.randint(0, self.num_classes, (batch_size,), device=next(self.model.parameters()).device)\n",
    "        return self.p_sample_loop(shape=(batch_size, channels, image_size, image_size), class_labels=class_labels)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# Training loop\n",
    "def train(ciddpm, dataloader, num_epochs, device, lr=2e-4):\n",
    "    optimizer = torch.optim.AdamW(ciddpm.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        ciddpm.train()\n",
    "        total_loss = 0\n",
    "        for batch, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            x = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            t = torch.randint(0, ciddpm.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            loss = ciddpm.p_losses(x, t, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "# Visualization\n",
    "def visualize_samples(ciddpm, num_samples, image_size, channels, device, class_labels=None):\n",
    "    ciddpm.eval()\n",
    "    if class_labels is None:\n",
    "        class_labels = torch.randint(0, ciddpm.num_classes, (num_samples,), device=device)\n",
    "    samples = ciddpm.sample(image_size=image_size, batch_size=num_samples, channels=channels, class_labels=class_labels)\n",
    "    samples = torch.from_numpy(samples[-1])  # Get the last step of the sampling process\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*3, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Class: {class_labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = 10  # CIFAR-10 has 10 classes\n",
    "    model = ConditionalImprovedUNet(num_classes=num_classes).to(device)\n",
    "    ciddpm = ConditionalImprovedDDPM(model, num_classes=num_classes).to(device)\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 100  # You might want to increase this for better results\n",
    "    train(ciddpm, dataloader, num_epochs, device)\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_samples(ciddpm, num_samples=5, image_size=32, channels=3, device=device)\n",
    "\n",
    "    # Generate samples for specific classes\n",
    "    class_labels = torch.tensor([0, 1, 2, 3, 4], device=device)  # Generate one sample for each of the first 5 classes\n",
    "    visualize_samples(ciddpm, num_samples=5, image_size=32, channels=3, device=device, class_labels=class_labels)\n",
    "\n",
    "    # Function to generate and save samples\n",
    "    def generate_and_save_samples(ciddpm, num_samples, image_size, channels, device, save_path):\n",
    "        ciddpm.eval()\n",
    "        class_labels = torch.arange(ciddpm.num_classes).repeat(num_samples).to(device)\n",
    "        samples = ciddpm.sample(image_size=image_size, batch_size=num_samples * ciddpm.num_classes, channels=channels, class_labels=class_labels)\n",
    "        samples = torch.from_numpy(samples[-1])  # Get the last step of the sampling process\n",
    "        \n",
    "        fig, axes = plt.subplots(ciddpm.num_classes, num_samples, figsize=(num_samples*3, ciddpm.num_classes*3))\n",
    "        for i in range(ciddpm.num_classes):\n",
    "            for j in range(num_samples):\n",
    "                img = samples[i*num_samples + j].permute(1, 2, 0).cpu().numpy()\n",
    "                img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].axis('off')\n",
    "            axes[i, 0].set_title(f\"Class {i}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    # Generate and save a grid of samples\n",
    "    generate_and_save_samples(ciddpm, num_samples=5, image_size=32, channels=3, device=device, save_path='ciddpm_samples.png')\n",
    "\n",
    "    print(\"Training and sampling completed. Check 'ciddpm_samples.png' for generated samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, class_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.class_mlp = nn.Linear(class_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, c):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        class_emb = self.relu(self.class_mlp(c))\n",
    "        h = h + time_emb[(..., ) + (None, ) * 2] + class_emb[(..., ) + (None, ) * 2]\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=4, time_emb_dim=32, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.class_emb = nn.Embedding(num_classes, time_emb_dim)\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.downs = nn.ModuleList([\n",
    "            Block(64, 128, time_emb_dim, time_emb_dim),\n",
    "            Block(128, 256, time_emb_dim, time_emb_dim),\n",
    "            Block(256, 256, time_emb_dim, time_emb_dim),\n",
    "        ])\n",
    "        self.ups = nn.ModuleList([\n",
    "            Block(256, 256, time_emb_dim, time_emb_dim, up=True),\n",
    "            Block(384, 128, time_emb_dim, time_emb_dim, up=True),\n",
    "            Block(192, 64, time_emb_dim, time_emb_dim, up=True),\n",
    "        ])\n",
    "        self.output = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, timestep, class_label):\n",
    "        t = self.time_mlp(timestep)\n",
    "        c = self.class_emb(class_label)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t, c)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t, c)\n",
    "        return self.output(x)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "class ConditionalLatentDiffusion(nn.Module):\n",
    "    def __init__(self, autoencoder, unet, num_classes, num_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.unet = unet\n",
    "        self.num_classes = num_classes\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        betas = cosine_beta_schedule(num_timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        \n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, x_start, t, class_labels, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        predicted_noise = self.unet(x_noisy, t, class_labels)\n",
    "\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_index, class_labels):\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            self.sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "        )\n",
    "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas_cumprod, t, x.shape)\n",
    "        \n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * self.unet(x, t, class_labels) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(self.betas, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, class_labels):\n",
    "        device = next(self.unet.parameters()).device\n",
    "\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='sampling loop time step', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long), i, class_labels)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, image_size, batch_size=16, channels=256, class_labels=None):\n",
    "        if class_labels is None:\n",
    "            class_labels = torch.randint(0, self.num_classes, (batch_size,), device=next(self.unet.parameters()).device)\n",
    "        return self.p_sample_loop(shape=(batch_size, channels, image_size // 8, image_size // 8), class_labels=class_labels)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# Training loop\n",
    "def train(cldm, autoencoder, dataloader, num_epochs, device, lr=1e-4):\n",
    "    optimizer = torch.optim.AdamW(cldm.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        cldm.train()\n",
    "        total_loss = 0\n",
    "        for batch, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            x = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Encode the input to latent space\n",
    "            with torch.no_grad():\n",
    "                x_latent = autoencoder.encode(x)\n",
    "            \n",
    "            t = torch.randint(0, cldm.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            loss = cldm.p_losses(x_latent, t, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "def visualize_samples(cldm, autoencoder, num_samples, image_size, channels, device, class_labels=None):\n",
    "    cldm.eval()\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    if class_labels is None:\n",
    "        class_labels = torch.randint(0, cldm.num_classes, (num_samples,), device=device)\n",
    "    latent_samples = cldm.sample(image_size=image_size, batch_size=num_samples, channels=channels, class_labels=class_labels)\n",
    "    latent_samples = torch.from_numpy(latent_samples[-1]).to(device)  # Get the last step of the sampling process\n",
    "    \n",
    "    # Decode the latent samples\n",
    "    with torch.no_grad():\n",
    "        samples = autoencoder.decode(latent_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples*3, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Class: {class_labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "    # Initialize models\n",
    "    autoencoder = Autoencoder().to(device)\n",
    "    unet = ConditionalUNet(in_channels=256, out_channels=256, num_classes=num_classes).to(device)\n",
    "    cldm = ConditionalLatentDiffusion(autoencoder, unet, num_classes=num_classes).to(device)\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder_optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "    num_epochs_ae = 10\n",
    "    for epoch in range(num_epochs_ae):\n",
    "        autoencoder.train()\n",
    "        total_loss = 0\n",
    "        for batch, _ in tqdm(dataloader, desc=f\"Autoencoder Epoch {epoch+1}/{num_epochs_ae}\"):\n",
    "            x = batch.to(device)\n",
    "            autoencoder_optimizer.zero_grad()\n",
    "            x_recon = autoencoder(x)\n",
    "            loss = F.mse_loss(x_recon, x)\n",
    "            loss.backward()\n",
    "            autoencoder_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Autoencoder Epoch {epoch+1}/{num_epochs_ae}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Train the conditional latent diffusion model\n",
    "    num_epochs_cldm = 100  # You might want to increase this for better results\n",
    "    train(cldm, autoencoder, dataloader, num_epochs_cldm, device)\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_samples(cldm, autoencoder, num_samples=5, image_size=32, channels=256, device=device)\n",
    "\n",
    "    # Generate samples for specific classes\n",
    "    class_labels = torch.tensor([0, 1, 2, 3, 4], device=device)  # Generate one sample for each of the first 5 classes\n",
    "    visualize_samples(cldm, autoencoder, num_samples=5, image_size=32, channels=256, device=device, class_labels=class_labels)\n",
    "\n",
    "    # Function to generate and save samples\n",
    "    def generate_and_save_samples(cldm, autoencoder, num_samples, image_size, channels, device, save_path):\n",
    "        cldm.eval()\n",
    "        autoencoder.eval()\n",
    "        class_labels = torch.arange(cldm.num_classes).repeat(num_samples).to(device)\n",
    "        latent_samples = cldm.sample(image_size=image_size, batch_size=num_samples * cldm.num_classes, channels=channels, class_labels=class_labels)\n",
    "        latent_samples = torch.from_numpy(latent_samples[-1]).to(device)  # Get the last step of the sampling process\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            samples = autoencoder.decode(latent_samples)\n",
    "        \n",
    "        fig, axes = plt.subplots(cldm.num_classes, num_samples, figsize=(num_samples*3, cldm.num_classes*3))\n",
    "        for i in range(cldm.num_classes):\n",
    "            for j in range(num_samples):\n",
    "                img = samples[i*num_samples + j].permute(1, 2, 0).cpu().numpy()\n",
    "                img = (img + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].axis('off')\n",
    "            axes[i, 0].set_title(f\"Class {i}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    # Generate and save a grid of samples\n",
    "    generate_and_save_samples(cldm, autoencoder, num_samples=5, image_size=32, channels=256, device=device, save_path='cldm_samples.png')\n",
    "\n",
    "    print(\"Training and sampling completed. Check 'cldm_samples.png' for generated samples.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
