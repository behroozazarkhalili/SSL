{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTBYOLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom MNIST dataset for BYOL that returns two augmented versions of each image.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, train: bool = True, download: bool = False):\n",
    "        self.mnist = datasets.MNIST(root, train=train, download=download)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(28, scale=(0.2, 1.0)),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        img, target = self.mnist[index]\n",
    "        return self.transform(img), self.transform(img), target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mnist)\n",
    "\n",
    "class EncoderNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network for BYOL.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim: int = 256):\n",
    "        super(EncoderNetwork, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(9216, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ProjectionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Projection network for BYOL.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 256, hidden_dim: int = 256, output_dim: int = 128):\n",
    "        super(ProjectionNetwork, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.projection(x)\n",
    "\n",
    "class PredictionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction network for BYOL.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.prediction(x)\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    \"\"\"\n",
    "    BYOL model combining encoder, projection, and prediction networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim: int = 256, projection_dim: int = 128):\n",
    "        super(BYOL, self).__init__()\n",
    "        self.online_encoder = EncoderNetwork(feature_dim)\n",
    "        self.online_projector = ProjectionNetwork(feature_dim, feature_dim, projection_dim)\n",
    "        self.online_predictor = PredictionNetwork(projection_dim, feature_dim, projection_dim)\n",
    "        \n",
    "        self.target_encoder = EncoderNetwork(feature_dim)\n",
    "        self.target_projector = ProjectionNetwork(feature_dim, feature_dim, projection_dim)\n",
    "        \n",
    "        # Initialize target network with online network's parameters\n",
    "        self.target_encoder.load_state_dict(self.online_encoder.state_dict())\n",
    "        self.target_projector.load_state_dict(self.online_projector.state_dict())\n",
    "        \n",
    "        # Freeze target network\n",
    "        for param in self.target_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.target_projector.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Online network forward pass\n",
    "        online_feat1 = self.online_encoder(x1)\n",
    "        online_proj1 = self.online_projector(online_feat1)\n",
    "        online_pred1 = self.online_predictor(online_proj1)\n",
    "        \n",
    "        online_feat2 = self.online_encoder(x2)\n",
    "        online_proj2 = self.online_projector(online_feat2)\n",
    "        online_pred2 = self.online_predictor(online_proj2)\n",
    "        \n",
    "        # Target network forward pass\n",
    "        with torch.no_grad():\n",
    "            target_feat1 = self.target_encoder(x1)\n",
    "            target_proj1 = self.target_projector(target_feat1)\n",
    "            \n",
    "            target_feat2 = self.target_encoder(x2)\n",
    "            target_proj2 = self.target_projector(target_feat2)\n",
    "        \n",
    "        return online_pred1, online_pred2, target_proj1.detach(), target_proj2.detach()\n",
    "\n",
    "def byol_loss(online_pred1: torch.Tensor, online_pred2: torch.Tensor, \n",
    "               target_proj1: torch.Tensor, target_proj2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute BYOL loss.\n",
    "    \"\"\"\n",
    "    loss1 = 2 - 2 * (online_pred1 * target_proj2).sum(dim=-1).mean()\n",
    "    loss2 = 2 - 2 * (online_pred2 * target_proj1).sum(dim=-1).mean()\n",
    "    return (loss1 + loss2) / 2\n",
    "\n",
    "def update_moving_average(online_model: nn.Module, target_model: nn.Module, beta: float = 0.99):\n",
    "    \"\"\"\n",
    "    Update moving average of target network.\n",
    "    \"\"\"\n",
    "    for online_params, target_params in zip(online_model.parameters(), target_model.parameters()):\n",
    "        target_params.data = beta * target_params.data + (1 - beta) * online_params.data\n",
    "\n",
    "def train_byol(model: BYOL, train_loader: DataLoader, optimizer: optim.Optimizer, \n",
    "               device: torch.device, epochs: int = 100) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the BYOL model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (x1, x2, _) in enumerate(train_loader):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            online_pred1, online_pred2, target_proj1, target_proj2 = model(x1, x2)\n",
    "            loss = byol_loss(online_pred1, online_pred2, target_proj1, target_proj2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            update_moving_average(model.online_encoder, model.target_encoder)\n",
    "            update_moving_average(model.online_projector, model.target_projector)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier for MNIST using the pre-trained BYOL encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: EncoderNetwork):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(256, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.encoder(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "def train_classifier(model: MNISTClassifier, train_loader: DataLoader, test_loader: DataLoader, \n",
    "                     optimizer: optim.Optimizer, device: torch.device, epochs: int = 10):\n",
    "    \"\"\"\n",
    "    Fine-tune the classifier on MNIST.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data1, data2, target) in enumerate(train_loader):\n",
    "            # Use only the first augmented view (data1) for classification\n",
    "            data, target = data1.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Test Accuracy: {accuracy:.2f}%\")\n",
    "        model.train()\n",
    "\n",
    "def extract_features(model: nn.Module, data_loader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract features using the trained encoder.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            feature = model.encoder(data)\n",
    "            features.append(feature.cpu().numpy())\n",
    "            labels.append(target.numpy())\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "def plot_embeddings(features: np.ndarray, labels: np.ndarray, method: str = 'UMAP'):\n",
    "    \"\"\"\n",
    "    Plot embeddings using UMAP or PCA.\n",
    "    \"\"\"\n",
    "    if method == 'UMAP':\n",
    "        reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    elif method == 'PCA':\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'UMAP' or 'PCA'\")\n",
    "    \n",
    "    embeddings = reducer.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='tab10', s=5, alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'{method} visualization of BYOL learned representations')\n",
    "    plt.xlabel(f'{method}_1')\n",
    "    plt.ylabel(f'{method}_2')\n",
    "    plt.savefig(f'byol_mnist_{method.lower()}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = MNISTBYOLDataset('data', train=True, download=True)\n",
    "    test_dataset = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize BYOL model\n",
    "    byol_model = BYOL().to(device)\n",
    "    optimizer = optim.Adam(byol_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train BYOL\n",
    "    print(\"Starting BYOL training...\")\n",
    "    losses = train_byol(byol_model, train_loader, optimizer, device, epochs=100)\n",
    "    \n",
    "    # Plot BYOL training loss\n",
    "    plt.plot(losses)\n",
    "    plt.title('BYOL Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('byol_training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Initialize and train classifier\n",
    "    classifier = MNISTClassifier(byol_model.online_encoder).to(device)\n",
    "    classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Starting classifier fine-tuning...\")\n",
    "    train_classifier(classifier, train_loader, test_loader, classifier_optimizer, device, epochs=10)\n",
    "    \n",
    "    # Extract features and create visualizations\n",
    "    features, labels = extract_features(classifier, test_loader, device)\n",
    "    \n",
    "    print(\"Generating UMAP visualization...\")\n",
    "    plot_embeddings(features, labels, method='UMAP')\n",
    "    \n",
    "    print(\"Generating PCA visualization...\")\n",
    "    plot_embeddings(features, labels, method='PCA')\n",
    "    \n",
    "    print(\"BYOL training and evaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
