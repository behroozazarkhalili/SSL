{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "\n",
    "class MNISTSimSiamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom MNIST dataset for SimSiam that returns two augmented versions of each image.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, train: bool = True, download: bool = False):\n",
    "        self.mnist = datasets.MNIST(root, train=train, download=download)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(28, scale=(0.2, 1.0)),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        img, target = self.mnist[index]\n",
    "        return self.transform(img), self.transform(img), target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mnist)\n",
    "\n",
    "class SimSiamMNIST(nn.Module):\n",
    "    \"\"\"\n",
    "    SimSiam model for MNIST.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim: int = 256, pred_dim: int = 128):\n",
    "        super(SimSiamMNIST, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(9216, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # Projection MLP\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # Prediction MLP\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(feature_dim, pred_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(pred_dim, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        z1 = self.projection(self.encoder(x1))\n",
    "        z2 = self.projection(self.encoder(x2))\n",
    "        \n",
    "        p1 = self.prediction(z1)\n",
    "        p2 = self.prediction(z2)\n",
    "        \n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "\n",
    "def simsiam_loss(p1: torch.Tensor, p2: torch.Tensor, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute SimSiam loss.\n",
    "    \"\"\"\n",
    "    # Negative cosine similarity\n",
    "    return - (nn.functional.cosine_similarity(p1, z2.detach(), dim=-1).mean() + \n",
    "              nn.functional.cosine_similarity(p2, z1.detach(), dim=-1).mean()) * 0.5\n",
    "\n",
    "def train_simsiam(model: SimSiamMNIST, train_loader: DataLoader, optimizer: optim.Optimizer, \n",
    "                  device: torch.device, epochs: int = 100) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the SimSiam model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (x1, x2, _) in enumerate(train_loader):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            p1, p2, z1, z2 = model(x1, x2)\n",
    "            loss = simsiam_loss(p1, p2, z1, z2)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier for MNIST using the pre-trained SimSiam encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: nn.Module):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(256, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.encoder(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "def train_classifier(model: MNISTClassifier, train_loader: DataLoader, test_loader: DataLoader, \n",
    "                     optimizer: optim.Optimizer, device: torch.device, epochs: int = 10):\n",
    "    \"\"\"\n",
    "    Fine-tune the classifier on MNIST.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, _, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Test Accuracy: {accuracy:.2f}%\")\n",
    "        model.train()\n",
    "\n",
    "def extract_features(model: nn.Module, data_loader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract features using the trained encoder.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            feature = model.encoder(data)\n",
    "            features.append(feature.cpu().numpy())\n",
    "            labels.append(target.numpy())\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "def plot_embeddings(features: np.ndarray, labels: np.ndarray, method: str = 'UMAP'):\n",
    "    \"\"\"\n",
    "    Plot embeddings using UMAP or PCA.\n",
    "    \"\"\"\n",
    "    if method == 'UMAP':\n",
    "        reducer = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    elif method == 'PCA':\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'UMAP' or 'PCA'\")\n",
    "    \n",
    "    embeddings = reducer.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='tab10', s=5, alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'{method} visualization of SimSiam learned representations')\n",
    "    plt.xlabel(f'{method}_1')\n",
    "    plt.ylabel(f'{method}_2')\n",
    "    plt.savefig(f'simsiam_mnist_{method.lower()}.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = MNISTSimSiamDataset('data', train=True, download=True)\n",
    "    test_dataset = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize SimSiam model\n",
    "    simsiam_model = SimSiamMNIST().to(device)\n",
    "    optimizer = optim.Adam(simsiam_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train SimSiam\n",
    "    print(\"Starting SimSiam training...\")\n",
    "    losses = train_simsiam(simsiam_model, train_loader, optimizer, device, epochs=10)\n",
    "    \n",
    "    # Plot SimSiam training loss\n",
    "    plt.plot(losses)\n",
    "    plt.title('SimSiam Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('simsiam_training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Initialize and train classifier\n",
    "    classifier = MNISTClassifier(simsiam_model.encoder).to(device)\n",
    "    classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Starting classifier fine-tuning...\")\n",
    "    train_classifier(classifier, train_loader, test_loader, classifier_optimizer, device, epochs=10)\n",
    "    \n",
    "    # Extract features and create visualizations\n",
    "    features, labels = extract_features(classifier, test_loader, device)\n",
    "    \n",
    "    print(\"Generating UMAP visualization...\")\n",
    "    plot_embeddings(features, labels, method='UMAP')\n",
    "    \n",
    "    print(\"Generating PCA visualization...\")\n",
    "    plot_embeddings(features, labels, method='PCA')\n",
    "    \n",
    "    print(\"SimSiam training and evaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
